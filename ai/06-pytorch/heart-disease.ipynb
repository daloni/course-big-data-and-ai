{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Dataset\n",
    "\n",
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array Shape: [ 52 125 212   0 168   0   1   2   0   1   0   1   0   0   0   0   1   0\n",
      "   0   0   1   0   0   0   1]\n",
      "Column Names: Index(['age', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak', 'ca',\n",
      "       'target', 'sex_male', 'sex_female', 'cp_type_0', 'cp_type_1',\n",
      "       'cp_type_2', 'cp_type_3', 'restecg_type_0', 'restecg_type_1',\n",
      "       'restecg_type_2', 'slope_type_0', 'slope_type_1', 'slope_type_2',\n",
      "       'thal_type_0', 'thal_type_1', 'thal_type_2', 'thal_type_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_heart_disease_dataset():\n",
    "    file_path = \"./data/heart.csv\"\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # One-hot encoding for field \"sex\"\n",
    "    data[\"sex_male\"] = (data[\"sex\"] == 1).astype(int)\n",
    "    data[\"sex_female\"] = (data[\"sex\"] == 0).astype(int)\n",
    "\n",
    "    # One-hot encoding for field \"cp\" (chest pain type)\n",
    "    cp_dummies = pd.get_dummies(data[\"cp\"], prefix=\"cp_type\")\n",
    "    data = pd.concat([data, cp_dummies], axis=1)\n",
    "\n",
    "    # One-hot encoding for field \"restecg\" (resting electrocardiographic results)\n",
    "    restecg_dummies = pd.get_dummies(data[\"restecg\"], prefix=\"restecg_type\")\n",
    "    data = pd.concat([data, restecg_dummies], axis=1)\n",
    "\n",
    "    # One-hot encoding for field \"slope\"\n",
    "    slope_dummies = pd.get_dummies(data[\"slope\"], prefix=\"slope_type\")\n",
    "    data = pd.concat([data, slope_dummies], axis=1)\n",
    "\n",
    "    # One-hot encoding for field \"thal\"\n",
    "    thal_dummies = pd.get_dummies(data[\"thal\"], prefix=\"thal_type\")\n",
    "    data = pd.concat([data, thal_dummies], axis=1)\n",
    "\n",
    "    # Transform all fields to int\n",
    "    data = data.astype(int)\n",
    "\n",
    "    # Remove original columns\n",
    "    data.drop(columns=[\"sex\", \"cp\", \"restecg\", \"slope\", \"thal\"], inplace=True)\n",
    "\n",
    "    return data.to_numpy(), data.columns\n",
    "\n",
    "array_data, columns = read_heart_disease_dataset()\n",
    "\n",
    "print(\"Array Shape:\", array_data[0])\n",
    "print(\"Column Names:\", columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeartDiseaseDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Divide el array en caracterÃ­sticas y etiquetas\n",
    "        self.X = torch.tensor(data[:, :-1], dtype=torch.float32)\n",
    "        self.y = torch.tensor(data[:, -1], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=300, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=300, out_features=64, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "array_data_length = len(array_data)\n",
    "test_data_items_length = int(array_data_length * 0.2)\n",
    "train_data_items_length = array_data_length - test_data_items_length\n",
    "\n",
    "dataset = HeartDiseaseDataset(array_data)\n",
    "train_data, test_data = random_split(dataset, [train_data_items_length, test_data_items_length])\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(24, 300),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(300, 300),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(300, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 5.665998  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 1.056060 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.147853  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.663513 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.781261  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.667884 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.696381  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.654748 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.631889  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.670897 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.713575  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.648870 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.686629  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.642535 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.677895  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.643878 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.701370  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.657399 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.647619  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.635942 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.711344  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.642733 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.624508  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.638435 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.688637  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.636746 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.643645  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.654186 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.678377  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.637369 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.640093  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.619744 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.656133  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.625454 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.684271  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.614354 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.633095  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.604776 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.670183  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.616913 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.663291  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.601281 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.652983  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.612133 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.647606  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.602119 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.651038  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.595758 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.627601  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.606919 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.662699  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.589820 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.586203  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.595365 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.626009  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.578480 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.603743  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.588397 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.653203  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.574234 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.648921  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.569300 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.613076  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.561682 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.648709  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.552100 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.579690  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.544510 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.613212  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.542244 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.555109  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.528622 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.552373  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.522379 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.578673  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.526336 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.650609  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.517466 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.536506  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.502412 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.536268  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.471594 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.529445  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.470142 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.511226  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.434596 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.452351  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.470477 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.494116  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.431219 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.508370  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.426425 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.475028  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.453164 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.669984  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.391124 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.504288  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.388921 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.396798  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.385678 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.417612  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.396710 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.499630  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.373535 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.387130  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.393134 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.389598  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.353645 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.520567  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.345869 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.454085  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.354688 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.500069  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.387846 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.532559  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.326316 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.370544  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.318630 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.420997  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.294590 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.377032  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.327776 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.352433  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.341681 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.352185  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.327276 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.345734  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.313967 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.404519  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.261995 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.315763  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.309578 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.392933  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.251471 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.368861  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.221973 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.198211  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.228088 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.256378  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.243664 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.503980  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.228779 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.256697  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.198720 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.205719  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.221716 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.287484  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.196259 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.272966  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.170575 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.238972  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.155872 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.263913  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.155678 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.214716  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 95.1%, Avg loss: 0.131752 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.182330  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.161375 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.364574  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 0.119709 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.253531  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 95.6%, Avg loss: 0.111039 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.121080  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 95.1%, Avg loss: 0.148188 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.191704  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 0.114084 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.180021  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.071556 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.121183  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 96.1%, Avg loss: 0.086505 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.175757  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.092607 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.106540  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 96.1%, Avg loss: 0.119506 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.089409  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 96.1%, Avg loss: 0.093823 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.140922  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.070393 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.124894  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.066945 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.104238  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.044438 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.039583  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.070342 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.188739  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.038414 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.121510  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 99.5%, Avg loss: 0.032117 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.060255  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 99.5%, Avg loss: 0.025157 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.049148  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 0.089611 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.138933  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.046291 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.141152  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 99.5%, Avg loss: 0.029198 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.071951  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.043952 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.132485  [   64/  820]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.023945 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 655 : tensor([ 41., 110., 235.,   0., 153.,   0.,   0.,   0.,   1.,   1.,   0.,   0.,\n",
      "          1.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   1.,   0.,   0.,   1.])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss_fn(pred, y)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtestRandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m, in \u001b[0;36mtestRandom\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElement\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_element, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X)\n\u001b[1;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([random_element])\n\u001b[0;32m---> 10\u001b[0m my_custom_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mHeartDiseaseDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(my_custom_dataset)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mHeartDiseaseDataset.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Divide el array en caracterÃ­sticas y etiquetas\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def testRandom(dataset):\n",
    "    random_element = random.randint(0, len(dataset))\n",
    "    X, Y = dataset[random_element]\n",
    "    print(\"Element\", random_element, \":\", X)\n",
    "\n",
    "    data = np.array([random_element])\n",
    "    my_custom_dataset = HeartDiseaseDataset(data)\n",
    "    dataloader = DataLoader(my_custom_dataset)\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        print(\"Loss:\", loss_fn(pred, y).item())\n",
    "\n",
    "testRandom(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
