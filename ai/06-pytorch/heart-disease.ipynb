{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Dataset\n",
    "\n",
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array Shape: [ 52 125 212   0 168   0   1   2   0   1   0   1   0   0   0   0   1   0\n",
      "   0   0   1   0   0   0   1]\n",
      "Column Names: Index(['age', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak', 'ca',\n",
      "       'target', 'sex_male', 'sex_female', 'cp_type_0', 'cp_type_1',\n",
      "       'cp_type_2', 'cp_type_3', 'restecg_type_0', 'restecg_type_1',\n",
      "       'restecg_type_2', 'slope_type_0', 'slope_type_1', 'slope_type_2',\n",
      "       'thal_type_0', 'thal_type_1', 'thal_type_2', 'thal_type_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_heart_disease_dataset():\n",
    "    file_path = \"./data/heart.csv\"\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # One-hot encoding for field \"sex\"\n",
    "    data[\"sex_male\"] = (data[\"sex\"] == 1).astype(int)\n",
    "    data[\"sex_female\"] = (data[\"sex\"] == 0).astype(int)\n",
    "\n",
    "    # One-hot encoding for field \"cp\" (chest pain type)\n",
    "    cp_dummies = pd.get_dummies(data[\"cp\"], prefix=\"cp_type\")\n",
    "    data = pd.concat([data, cp_dummies], axis=1)\n",
    "\n",
    "    # One-hot encoding for field \"restecg\" (resting electrocardiographic results)\n",
    "    restecg_dummies = pd.get_dummies(data[\"restecg\"], prefix=\"restecg_type\")\n",
    "    data = pd.concat([data, restecg_dummies], axis=1)\n",
    "\n",
    "    # One-hot encoding for field \"slope\"\n",
    "    slope_dummies = pd.get_dummies(data[\"slope\"], prefix=\"slope_type\")\n",
    "    data = pd.concat([data, slope_dummies], axis=1)\n",
    "\n",
    "    # One-hot encoding for field \"thal\"\n",
    "    thal_dummies = pd.get_dummies(data[\"thal\"], prefix=\"thal_type\")\n",
    "    data = pd.concat([data, thal_dummies], axis=1)\n",
    "\n",
    "    # Transform all fields to int\n",
    "    data = data.astype(int)\n",
    "\n",
    "    # Remove original columns\n",
    "    data.drop(columns=[\"sex\", \"cp\", \"restecg\", \"slope\", \"thal\"], inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = read_heart_disease_dataset()\n",
    "\n",
    "array_data = data.to_numpy()\n",
    "columns = data.columns\n",
    "\n",
    "print(\"Array Shape:\", array_data[0])\n",
    "print(\"Column Names:\", columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sum of input lengths does not equal the length of the input dataset!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTensor\n\u001b[0;32m----> 7\u001b[0m training_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(training_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     10\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n",
      "File \u001b[0;32m~/Documentos/course/ai/venv-cuda/lib/python3.10/site-packages/torch/utils/data/dataset.py:480\u001b[0m, in \u001b[0;36mrandom_split\u001b[0;34m(dataset, lengths, generator)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Cannot verify that dataset is Sized\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset):  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of input lengths does not equal the length of the input dataset!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m     )\n\u001b[1;32m    484\u001b[0m indices \u001b[38;5;241m=\u001b[39m randperm(\u001b[38;5;28msum\u001b[39m(lengths), generator\u001b[38;5;241m=\u001b[39mgenerator)\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# type: ignore[arg-type, call-overload]\u001b[39;00m\n\u001b[1;32m    485\u001b[0m lengths \u001b[38;5;241m=\u001b[39m cast(Sequence[\u001b[38;5;28mint\u001b[39m], lengths)\n",
      "\u001b[0;31mValueError\u001b[0m: Sum of input lengths does not equal the length of the input dataset!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data, test_data = random_split(array_data, [1200, 600])\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
